# Import our dependencies

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler,OneHotEncoder
import pandas as pd
import tensorflow as tf

#  Import and read the charity_data.csv.

import pandas as pd 
application_df = pd.read_csv("../Resources/charity_data.csv")
application_df.head()

# Drop the non-beneficial ID columns, 'EIN' and 'NAME'.

clean_application_df = application_df.drop(columns=['EIN', 'NAME'])
clean_application_df.head()

# Determine the number of unique values in each column.

df_categories = clean_application_df.dtypes.index.tolist()
clean_application_df[df_categories].nunique()

# Look at APPLICATION_TYPE value counts for binning

application_counts = clean_application_df.APPLICATION_TYPE.value_counts()
application_counts

# Visualize the value counts of APPLICATION_TYPE

application_counts.plot.density()

# Determine which values to replace if counts are less than 500?

replace_application = list(application_counts[application_counts < 500].index)

# Replace in dataframe

for app in replace_application:
    clean_application_df.APPLICATION_TYPE = clean_application_df.APPLICATION_TYPE.replace(app,"Other")
    
# Check to make sure binning was successful

clean_application_df.APPLICATION_TYPE.value_counts()

# Look at CLASSIFICATION value counts for binning

classification_counts = clean_application_df.CLASSIFICATION.value_counts()
classification_counts

# Visualize the value counts of CLASSIFICATION

classification_counts.plot.density()

# Determine which values to replace if counts are less than 1800?

replace_class = list(classification_counts[classification_counts < 1800].index)

# Replace in dataframe

for cls in replace_class:
    clean_application_df.CLASSIFICATION = clean_application_df.CLASSIFICATION.replace(cls,"Other")
    
# Check to make sure binning was successful

clean_application_df.CLASSIFICATION.value_counts()

# Generate our categorical variable lists

application_cat = clean_application_df.dtypes[clean_application_df.dtypes == "object"].index.tolist()

# Create a OneHotEncoder instance

enc = OneHotEncoder(sparse=False)

# Fit and transform the OneHotEncoder using the categorical variable list

encode_df = pd.DataFrame(enc.fit_transform(clean_application_df[application_cat]))

# Add the encoded variable names to the dataframe

encode_df.columns = enc.get_feature_names_out(application_cat)
encode_df.head()

# Merge one-hot encoded features and drop the originals

clean_application_df = clean_application_df.merge(encode_df,left_index=True, right_index=True)
clean_application_df = clean_application_df.drop(application_cat,1)
clean_application_df.head()

# Split our preprocessed data into our features and target arrays

y = clean_application_df["IS_SUCCESSFUL"].values
X = clean_application_df.drop(["IS_SUCCESSFUL"],1).values

# Split the preprocessed data into a training and testing dataset

X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)

# Create a StandardScaler instances

scaler = StandardScaler()

# Fit the StandardScaler

X_scaler = scaler.fit(X_train)

# Scale the data

X_train_scaled = X_scaler.transform(X_train)
X_test_scaled = X_scaler.transform(X_test)

# Define the model - deep neural net, i.e., the number of input features and hidden nodes for each layer.

number_input_features = len(X_train[0])
hidden_nodes_layer1 = 80
hidden_nodes_layer2 = 30

nn = tf.keras.models.Sequential()

# First hidden layer

nn.add(
    tf.keras.layers.Dense(units=hidden_nodes_layer1, input_dim=number_input_features, activation="relu")
)

# Second hidden layer

nn.add(tf.keras.layers.Dense(units=hidden_nodes_layer2, activation="relu"))

# Output layer

nn.add(tf.keras.layers.Dense(units=1, activation="sigmoid"))

# Check the structure of the model

nn.summary()

# Compile the model

nn.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])

# Import checkpoint dependencies

from tensorflow.keras.callbacks import ModelCheckpoint

checkpoint_path = "checkpoints/weights.{epoch:02d}.hdf5"

# Create a callback that saves the model's weights every epoch

cp_callback = ModelCheckpoint(
    filepath=checkpoint_path,
    verbose=1,
    save_weights_only=True,
    period=5)

# Train the model

fit_model = nn.fit(X_train_scaled,y_train,epochs=100,callbacks=[cp_callback])

# Evaluate the model using the test data

model_loss, model_accuracy = nn.evaluate(X_test_scaled,y_test,verbose=2)
print(f"Loss: {model_loss}, Accuracy: {model_accuracy}")

# Export our model to HDF5 file

nn.save("AlphabetSoupCharity.h5")